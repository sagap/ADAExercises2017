{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'Data'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline / Methodology\n",
    "\n",
    "#### Data organisation\n",
    "The data for this task is organised in three directories, one for each country. Within each directory there are the virus data for the corresponding country, split to several CSV files, one for each day of a month (all data files concern year 2014).\n",
    "\n",
    "By looking at the data files, we observe that the periods they cover are not the same. For example, the data for Guinea cover most days of September and just a few days of August, the data for Liberia cover the period from June to December, while the data for Sierra Leone cover the period from August to December. Futhermore, the periods are not continuous, i.e. there are days for which data files are missing. These facts should not affect our analysis.\n",
    "\n",
    "#### Methodology\n",
    "For reasons that will become clear later, but that are mostly attributed to missing data, we will aggregate total (suspected+probable+confirmed) cases and deaths. The idea behind this decision is that in cases that data is missing for one of the three categories, we just aggregate the data available.\n",
    "\n",
    "The first step for each country is to normalise their datasets, namely bring them to a common format, which we will be able to process later in a general way. This normalisation runs some checks on the data and tries to infer and fill in some missing data. For example, some Liberia data files contain the individual numbers for suspected, probable and confirmed cases or deaths, but not the total. In places where it can be safely infered, we fill this value in.\n",
    "\n",
    "Furthermore, we make checks to find out which categories it is safe to use for the data aggregation. For example, if we do not have enough data for a category, we do not use it (in the sense that we will pick some other variable for the final `DataFrame`). If a category makes no sense (e.g. the end-start of month difference of cumulative sums is negative - that is we resurrected people), we don't use it either.\n",
    "\n",
    "#### Daily vs. Cumulative data\n",
    "When considering the required data analysis, we can point out two possible ways of carrying it out:\n",
    "1. Calculate the mean of daily cases/deaths. This approach will work well if the data indicating daily cases/deaths is consistent and correct. If many such values are missing or, even worse, are falselly zero, the average will suffer. As we will see, the daily data for all three countries is generally either missing, or inconsistent.\n",
    "2. Aggregate as much cumulative data as possible for the available periods. Then, calculate the difference of the cumulative values for a given month and divide by the number of days available for that month. This a way safer way of calculating things, as we do not depend of each individual day data. If, say, a day's data slipped, this will not affect the overall calculation. Or if, say, a correction was made, we don't need to bother with it, as it will be reflected to the rest of the data.\n",
    "\n",
    "Our choice was to go with the second approach. The reason will become more clear, as soon as we present the analysis of the data for each country.\n",
    "\n",
    "We begin by importing some modules and defining some constants..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "columns_to_check_cases = [\n",
    "    'ADA_NEW_CASES_SUSPECT',\n",
    "    'ADA_NEW_CASES_PROBABLE',\n",
    "    'ADA_NEW_CASES_CONFIRMED',\n",
    "    'ADA_NEW_CASES_TOTAL',\n",
    "    'ADA_CUM_CASES_SUSPECT',\n",
    "    'ADA_CUM_CASES_PROBABLE',\n",
    "    'ADA_CUM_CASES_CONFIRMED',\n",
    "    'ADA_CUM_CASES_TOTAL',\n",
    "]\n",
    "\n",
    "columns_to_check_deaths = [\n",
    "    'ADA_NEW_DEATHS_SUSPECT',\n",
    "    'ADA_NEW_DEATHS_PROBABLE',\n",
    "    'ADA_NEW_DEATHS_CONFIRMED',\n",
    "    'ADA_NEW_DEATHS_TOTAL',\n",
    "    'ADA_CUM_DEATHS_SUSPECT',\n",
    "    'ADA_CUM_DEATHS_PROBABLE',\n",
    "    'ADA_CUM_DEATHS_CONFIRMED',\n",
    "    'ADA_CUM_DEATHS_TOTAL',\n",
    "]\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** We normalise Guinea dataset. As you can see from the execution of the code in the following cell, for the Guinea dataset it is safe to use both the new and cumulative totals. However, if we want to average on individual categories, some data is missing. For example, the daily new **confirmed** deaths values exists in only 1 out of 22 days. It is clear that we cannot rely on that category data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(DATA_FOLDER, 'ebola', 'guinea_data', '*.csv'))\n",
    "columns = ['Date', 'Description', 'Totals']\n",
    "gdf = pd.concat(pd.read_csv(f, parse_dates=['Date'],\n",
    "                            usecols=columns,\n",
    "                            index_col=['Date'])\n",
    "                for f in files)\n",
    "\n",
    "# Sort resulting DataFrame by index (Date)\n",
    "gdf.sort_index(inplace=True)\n",
    "# Convert Totals column to floats.\n",
    "gdf['Totals'] = gdf['Totals'].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Number of days in Guinea dataset.\n",
    "number_of_days_in_dataset = len(gdf.index.unique())\n",
    "\n",
    "# Normalise columns.\n",
    "conversions = (\n",
    "    # New cases\n",
    "    (r\"^new.*cases.*suspects$\", 'ADA_NEW_CASES_SUSPECT'),\n",
    "    (r\"^new.*cases.*probables$\", 'ADA_NEW_CASES_PROBABLE'),\n",
    "    (r\"^new.*cases.*confirmed$\", 'ADA_NEW_CASES_CONFIRMED'),\n",
    "    (r\"^total.*new.*cases\", 'ADA_NEW_CASES_TOTAL'),\n",
    "    # Cumulative cases\n",
    "    (r\"^total.*cases.*suspects\", 'ADA_CUM_CASES_SUSPECT'),\n",
    "    (r\"^total.*cases.*probables\", 'ADA_CUM_CASES_PROBABLE'),\n",
    "    (r\"^total.*cases.*confirmed\", 'ADA_CUM_CASES_CONFIRMED'),\n",
    "    (r\"^cumulative.*confirmed.*probable.*suspects\", 'ADA_CUM_CASES_TOTAL'),\n",
    "    # New deaths\n",
    "    (r\"^new.*deaths.*confirmed\", 'ADA_NEW_DEATHS_SUSPECT'),\n",
    "    (r\"^new.*deaths.*probables\", 'ADA_NEW_DEATHS_PROBABLE'),\n",
    "    (r\"^new.*deaths.*confirmed\", 'ADA_NEW_DEATHS_CONFIRMED'),\n",
    "    (r\"^new.*deaths.*registered( today)?$\", 'ADA_NEW_DEATHS_TOTAL'),\n",
    "    # Cumulative deaths\n",
    "    (r\"^total.*deaths.*suspects$\", 'ADA_CUM_DEATHS_SUSPECT'),\n",
    "    (r\"^total.*deaths.*probables$\", 'ADA_CUM_DEATHS_PROBABLE'),\n",
    "    (r\"^total.*deaths.*confirmed$\", 'ADA_CUM_DEATHS_CONFIRMED'),\n",
    "    (r\"^total.*deaths.*confirmed.*probable.*suspects\", 'ADA_CUM_DEATHS_TOTAL'),\n",
    ")\n",
    "\n",
    "print(\"Normalising columns for Guinea dataset...\", end=' ')\n",
    "for regex, val in conversions:\n",
    "    gdf.loc[[re.match(regex, x.lower()) is not None for x in gdf['Description']], 'Description'] = val\n",
    "print(\"Done!\", end='\\n\\n')\n",
    "\n",
    "# Keep only ADA columns.\n",
    "gdf = gdf.loc[[x.startswith(\"ADA\") for x in gdf['Description']]]\n",
    "\n",
    "# Make some checks.\n",
    "print(\"Checking columns of interest for Guinea dataset...\")\n",
    "print()\n",
    "print(\"Checking case-related columns:\")\n",
    "for col in columns_to_check_cases:\n",
    "    avail = gdf[(gdf.Description == col)].count()['Totals']\n",
    "    ind = '*' if avail == number_of_days_in_dataset else ' '\n",
    "    print('[{ind}] {col} ({avail}/{total})'.format(ind=ind, col=col, avail=avail, total=number_of_days_in_dataset))\n",
    "print()\n",
    "print(\"Checking death-related columns:\")\n",
    "for col in columns_to_check_deaths:\n",
    "    avail = gdf[(gdf.Description == col)].count()['Totals']\n",
    "    ind = '*' if avail == number_of_days_in_dataset else ' '\n",
    "    print('[{ind}] {col} ({avail}/{total})'.format(ind=ind, col=col, avail=avail, total=number_of_days_in_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** We normalise the Liberia dataset. The below code snippet does two jobs:\n",
    "1. Analyses the data and determines how complete it is, just like with the Guinea data. The output reveals that the Liberia data is way more sparse than the Guinea data, i.e. there is a lot of data missing. For example, there is not indication for daily deaths per individual caregory whatsoever and just half of the samples contain data for the cumulative deaths per individual category. It is clear that we have to average over cumulative totals.\n",
    "\n",
    "2. Taking into account point 1, tries to fill in some data, so that we can do the analysis. The data might be sparse, but there is something we can exploit. The cumulative data semantics. Since we are averaging per month, even if a lot of \"intermediate\" cumulative data is missing, we would be very happy if we had the cumulative data for the first and the last day of a month. We can then subtract and divide by the length of the period (days within the month that the period spans). This will give us a good approximation of the average we are after.\n",
    "\n",
    "Talking more specifically, the code tries to fill in the cumulative cases total, by adding together the individual cumulative values, where they are available. It need to do so only for the first and last day of a given month's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(DATA_FOLDER, 'ebola', 'liberia_data', '*.csv'))\n",
    "columns = ['Date', 'Variable', 'National']\n",
    "ldf = pd.concat(pd.read_csv(f, parse_dates=['Date'],\n",
    "                            usecols=columns,\n",
    "                            index_col=['Date'])\n",
    "                for f in files)\n",
    "\n",
    "# Sort resulting DataFrame by index (Date)\n",
    "ldf.sort_index(inplace=True)\n",
    "# Convert Totals column to floats.\n",
    "ldf['National'] = ldf['National'].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Number of days in Liberia dataset.\n",
    "number_of_days_in_dataset = len(ldf.index.unique())\n",
    "\n",
    "# Normalise columns.\n",
    "conversions = (\n",
    "    # New cases\n",
    "    (r\"^new.case.s..suspected.$\", 'ADA_NEW_CASES_SUSPECT'),\n",
    "    (r\"^new.case.s..probable.$\", 'ADA_NEW_CASES_PROBABLE'),\n",
    "    (r\"^^new.case.s..confirmed.$\", 'ADA_NEW_CASES_CONFIRMED'),\n",
    "    # Cumulative cases\n",
    "    (r\"^total.suspected.cases$\", 'ADA_CUM_CASES_SUSPECT'),\n",
    "    (r\"^total.probable.cases$\", 'ADA_CUM_CASES_PROBABLE'),\n",
    "    (r\"^total.confirmed.cases$\", 'ADA_CUM_CASES_CONFIRMED'),\n",
    "    (r\"^cumulative.*confirmed.*probable.*and.*suspected.*cases$\", 'ADA_CUM_CASES_TOTAL'),\n",
    "    # New deaths\n",
    "    (r\"^newly.reported.deaths$$\", 'ADA_NEW_DEATHS_TOTAL'),\n",
    "    # Cumulative deaths\n",
    "    (r\"^total.death.s.in.suspected.cases$\", 'ADA_CUM_DEATHS_SUSPECT'),\n",
    "    (r\"^total.death.s.in.probable.cases$\", 'ADA_CUM_DEATHS_PROBABLE'),\n",
    "    (r\"^total.death.s.in.confirmed.cases$\", 'ADA_CUM_DEATHS_CONFIRMED'),\n",
    "    (r\"^total.death.s.in.confirmed(.|\\n)*probable.*suspected.cases$\", 'ADA_CUM_DEATHS_TOTAL'),\n",
    ")\n",
    "\n",
    "print(\"Normalising columns for Liberia dataset...\", end=' ')\n",
    "for regex, val in conversions:\n",
    "    ldf.loc[[re.match(regex, x.lower()) is not None for x in ldf['Variable']], 'Variable'] = val\n",
    "print(\"Done!\", end='\\n\\n')\n",
    "\n",
    "# Keep only ADA columns.\n",
    "ldf = ldf.loc[[x.startswith(\"ADA\") for x in ldf['Variable']]]\n",
    "\n",
    "# Make some checks.\n",
    "print(\"Checking columns of interest for Liberia dataset...\")\n",
    "print()\n",
    "print(\"Checking case-related columns:\")\n",
    "for col in columns_to_check_cases:\n",
    "    avail = ldf[(ldf.Variable == col)].count()['National']\n",
    "    ind = '*' if avail == number_of_days_in_dataset else ' '\n",
    "    print('[{ind}] {col} ({avail}/{total})'.format(ind=ind, col=col, avail=avail, total=number_of_days_in_dataset))\n",
    "print()\n",
    "print(\"Checking death-related columns:\")\n",
    "for col in columns_to_check_deaths:\n",
    "    avail = ldf[(ldf.Variable == col)].count()['National']\n",
    "    ind = '*' if avail == number_of_days_in_dataset else ' '\n",
    "    print('[{ind}] {col} ({avail}/{total})'.format(ind=ind, col=col, avail=avail, total=number_of_days_in_dataset))\n",
    "\n",
    "\n",
    "# Create a DataFrame to put new rows. Will be later appended to main Liberia DataFrame.\n",
    "extra_df = pd.DataFrame(data={}, index=[], columns=['Variable', 'National'])\n",
    "\n",
    "def fill_missing_totals(group):\n",
    "    \"\"\"Fill the missing totals for a given group.\"\"\"\n",
    "    start_key = group.iloc[[0]].index[0]\n",
    "    end_key = group.iloc[[-1]].index[0]\n",
    "    \n",
    "    # Check if we can fill some TOTAL data for the first and last days of the month...\n",
    "    for key in [start_key, end_key]:\n",
    "        try:\n",
    "            start_cases_total = group.loc[group.Variable == 'ADA_CUM_CASES_TOTAL'].loc[key]['National']\n",
    "        except KeyError:\n",
    "            start_cases_total = 0\n",
    "            try:\n",
    "                start_cases_suspect = group.loc[group.Variable == 'ADA_CUM_CASES_SUSPECT'].loc[key]['National']\n",
    "                start_cases_total += start_cases_suspect\n",
    "            except KeyError: pass\n",
    "            try:\n",
    "                start_cases_probable = group.loc[group.Variable == 'ADA_CUM_CASES_PROBABLE'].loc[key]['National']\n",
    "                start_cases_total += start_cases_probable\n",
    "            except KeyError: pass\n",
    "            try:\n",
    "                start_cases_confirmed = group.loc[group.Variable == 'ADA_CUM_CASES_CONFIRMED'].loc[key]['National']\n",
    "                start_cases_total += start_cases_confirmed\n",
    "            except KeyError: pass\n",
    "            extra_df.loc[key] = ['ADA_CUM_CASES_TOTAL', start_cases_total]\n",
    "    \n",
    "\n",
    "def check_monthly_totals(group):\n",
    "    \"\"\"Checks whether we can make statistics with monthly totals.\"\"\"\n",
    "    totals = [\n",
    "        'ADA_CUM_CASES_SUSPECT',\n",
    "        'ADA_CUM_CASES_PROBABLE',\n",
    "        'ADA_CUM_CASES_CONFIRMED',\n",
    "        'ADA_CUM_CASES_TOTAL',\n",
    "        'ADA_CUM_DEATHS_SUSPECT',\n",
    "        'ADA_CUM_DEATHS_PROBABLE',\n",
    "        'ADA_CUM_DEATHS_CONFIRMED',\n",
    "        'ADA_CUM_DEATHS_TOTAL',\n",
    "    ]\n",
    "    \n",
    "    start_key = group.iloc[[0]].index[0]\n",
    "    end_key = group.iloc[[-1]].index[0]\n",
    "    print()\n",
    "    print(group.name, ':', start_key.strftime(\"%Y-%m-%d\"), '-', end_key.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    for tot in totals:\n",
    "        try:\n",
    "            start_total = group.loc[group.Variable == tot].loc[start_key]['National']\n",
    "            end_total = group.loc[group.Variable == tot].loc[end_key]['National']\n",
    "            ind = '*' if end_total > start_total else ' '\n",
    "        except KeyError as e:\n",
    "            ind = ' '\n",
    "        print('[{ind}] {total} ({res})'.format(ind=ind, total=tot, res=end_total-start_total))\n",
    "\n",
    "print()\n",
    "print(\"Data is too sparse, will abandon averaging over daily data.\")\n",
    "print(\"Checking what we can do with totals...\")\n",
    "ldf.groupby(by=lambda x: x.strftime(\"%Y-%m\")).apply(fill_missing_totals)\n",
    "# extra_df.index = pd.to_datetime(extra_df.index)\n",
    "ldf = ldf.append(extra_df).sort_index()\n",
    "ldf.groupby(by=lambda x: x.strftime(\"%Y-%m\")).apply(check_monthly_totals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** We normalise the Sierra Leone dataset. We make the following observations:\n",
    "1. There seem to be enough case data (daily and cumulative). The totals for those are missing, but they can be easily inferred.\n",
    "2. Regarding the death data, there is no daily data whatsoever, so we cannot use daily values for the aggregation. The good thing is that there are cumulative data, we just need to infer the total.\n",
    "\n",
    "We follow roughly the same path as with the Liberia dataset. We aggregate as much data as possible for the cumulative totals. We will calculate the average based on the cumulative total difference from the end of a month to the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(DATA_FOLDER, 'ebola', 'sl_data', '*.csv'))\n",
    "columns = ['date', 'variable', 'National']\n",
    "sdf = pd.concat(pd.read_csv(f, parse_dates=['date'],\n",
    "                            usecols=columns,\n",
    "                            index_col=['date'])\n",
    "                for f in files)\n",
    "\n",
    "# Sort resulting DataFrame by index (Date)\n",
    "sdf.sort_index(inplace=True)\n",
    "# Convert Totals column to floats.\n",
    "sdf['National'] = sdf['National'].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Number of days in Sierra Leone dataset.\n",
    "number_of_days_in_dataset = len(sdf.index.unique())\n",
    "\n",
    "# Normalise columns.\n",
    "conversions = (\n",
    "    # New cases\n",
    "    (r\"^new_suspected$\", 'ADA_NEW_CASES_SUSPECT'),\n",
    "    (r\"^new_probable$\", 'ADA_NEW_CASES_PROBABLE'),\n",
    "    (r\"^new_confirmed$\", 'ADA_NEW_CASES_CONFIRMED'),\n",
    "    # Cumulative cases\n",
    "    (r\"^cum_suspected$\", 'ADA_CUM_CASES_SUSPECT'),\n",
    "    (r\"^cum_probable$\", 'ADA_CUM_CASES_PROBABLE'),\n",
    "    (r\"^cum_confirmed$\", 'ADA_CUM_CASES_CONFIRMED'),\n",
    "    # Cumulative deaths\n",
    "    (r\"^death_suspected$\", 'ADA_CUM_DEATHS_SUSPECT'),\n",
    "    (r\"^death_probable$\", 'ADA_CUM_DEATHS_PROBABLE'),\n",
    "    (r\"^death_confirmed$\", 'ADA_CUM_DEATHS_CONFIRMED'),\n",
    ")\n",
    "\n",
    "print(\"Normalising columns for Sierra Leone dataset...\", end=' ')\n",
    "for regex, val in conversions:\n",
    "    sdf.loc[[re.match(regex, x.lower()) is not None for x in sdf['variable']], 'variable'] = val\n",
    "print(\"Done!\", end='\\n\\n')\n",
    "\n",
    "# Keep only ADA columns.\n",
    "sdf = sdf.loc[[x.startswith(\"ADA\") for x in sdf['variable']]]\n",
    "\n",
    "# Make some checks.\n",
    "print(\"Checking columns of interest for Sierra Leone dataset...\")\n",
    "print()\n",
    "print(\"Checking case-related columns:\")\n",
    "for col in columns_to_check_cases:\n",
    "    avail = sdf[(sdf.variable == col)].count()['National']\n",
    "    ind = '*' if avail == number_of_days_in_dataset else ' '\n",
    "    print('[{ind}] {col} ({avail}/{total})'.format(ind=ind, col=col, avail=avail, total=number_of_days_in_dataset))\n",
    "print()\n",
    "print(\"Checking death-related columns:\")\n",
    "for col in columns_to_check_deaths:\n",
    "    avail = sdf[(sdf.variable == col)].count()['National']\n",
    "    ind = '*' if avail == number_of_days_in_dataset else ' '\n",
    "    print('[{ind}] {col} ({avail}/{total})'.format(ind=ind, col=col, avail=avail, total=number_of_days_in_dataset))\n",
    "\n",
    "# Create two DataFrame to put new rows. Will be later appended to main Liberia DataFrame.\n",
    "# Here we need two DataFrames in because we will aggregate data from two different categories.\n",
    "extra_df = {\n",
    "    'ADA_CUM_CASES_TOTAL': pd.DataFrame(data={}, index=[], columns=['variable', 'National']),\n",
    "    'ADA_CUM_DEATHS_TOTAL': pd.DataFrame(data={}, index=[], columns=['variable', 'National']),\n",
    "}\n",
    "\n",
    "def fill_missing_totals(group):\n",
    "    \"\"\"Fill the missing totals for a given group.\"\"\"\n",
    "    start_key = group.iloc[[0]].index[0]\n",
    "    end_key = group.iloc[[-1]].index[0]\n",
    "    \n",
    "    # Check if we can fill some TOTAL data for the first and last days of the month...\n",
    "    for sus_var, prob_var, conf_var, tot_var in [\n",
    "        ['ADA_CUM_CASES_SUSPECT', 'ADA_CUM_CASES_PROBABLE', 'ADA_CUM_CASES_CONFIRMED', 'ADA_CUM_CASES_TOTAL'],\n",
    "        ['ADA_CUM_DEATHS_SUSPECT', 'ADA_CUM_DEATHS_PROBABLE', 'ADA_CUM_DEATHS_CONFIRMED', 'ADA_CUM_DEATHS_TOTAL'],\n",
    "    ]:\n",
    "        for key in [start_key, end_key]:\n",
    "            try:\n",
    "                start_cases_total = group.loc[group.variable == tot_var].loc[key]['National']\n",
    "            except KeyError:\n",
    "                start_cases_total = 0\n",
    "                try:\n",
    "                    start_cases_suspect = group.loc[group.variable == sus_var].loc[key]['National']\n",
    "                    start_cases_total += start_cases_suspect if pd.notnull(start_cases_suspect) else 0.0\n",
    "                except KeyError: pass\n",
    "                try:\n",
    "                    start_cases_probable = group.loc[group.variable == prob_var].loc[key]['National']\n",
    "                    start_cases_total += start_cases_probable if pd.notnull(start_cases_probable) else 0.0\n",
    "                except KeyError: pass\n",
    "                try:\n",
    "                    start_cases_confirmed = group.loc[group.variable == conf_var].loc[key]['National']\n",
    "                    start_cases_total += start_cases_confirmed if pd.notnull(start_cases_confirmed) else 0.0\n",
    "                except KeyError: pass\n",
    "                extra_df[tot_var].loc[key] = [tot_var, start_cases_total]\n",
    "\n",
    "def check_monthly_totals(group):\n",
    "    \"\"\"Checks whether we can make statistics with monthly totals.\"\"\"\n",
    "    totals = [\n",
    "        'ADA_CUM_CASES_SUSPECT',\n",
    "        'ADA_CUM_CASES_PROBABLE',\n",
    "        'ADA_CUM_CASES_CONFIRMED',\n",
    "        'ADA_CUM_CASES_TOTAL',\n",
    "        'ADA_CUM_DEATHS_SUSPECT',\n",
    "        'ADA_CUM_DEATHS_PROBABLE',\n",
    "        'ADA_CUM_DEATHS_CONFIRMED',\n",
    "        'ADA_CUM_DEATHS_TOTAL',\n",
    "    ]\n",
    "    \n",
    "    start_key = group.iloc[[0]].index[0]\n",
    "    end_key = group.iloc[[-1]].index[0]\n",
    "    print()\n",
    "    print(group.name, ':', start_key.strftime(\"%Y-%m-%d\"), '-', end_key.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    for tot in totals:\n",
    "        try:\n",
    "            start_total = group.loc[group.variable == tot].loc[start_key]['National']\n",
    "            end_total = group.loc[group.variable == tot].loc[end_key]['National']\n",
    "            ind = '*' if end_total > start_total else ' '\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "            ind = ' '\n",
    "        print('[{ind}] {total} ({res})'.format(ind=ind, total=tot, res=end_total-start_total))\n",
    "\n",
    "        \n",
    "# Delete the last day of December, as it makes the cumulative difference negative\n",
    "# (due to missing data).\n",
    "sdf.drop(sdf[sdf.index.strftime(\"%Y-%m-%d\") == '2014-12-13'].index, inplace=True)\n",
    "        \n",
    "print()\n",
    "print(\"Data is too sparse, will abandon averaging over daily data.\")\n",
    "print(\"Checking what we can do with totals...\")\n",
    "sdf.groupby(by=lambda x: x.strftime(\"%Y-%m\")).apply(fill_missing_totals)\n",
    "\n",
    "# Merge the two extra DataFrames with the main one.\n",
    "for _, df in extra_df.items():\n",
    "    sdf = sdf.append(df).sort_index()\n",
    "\n",
    "sdf.groupby(by=lambda x: x.strftime(\"%Y-%m\")).apply(check_monthly_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all normalised data in three `DataFrame`s, namely `gdf` for Guinea, `ldf` for Liberia and `sdf` for Sierra Leone. In order to calculate the averages we will `groupby` month and apply the `average_on_totals` function, defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate averages.\n",
    "def average_on_totals_with_daily(group):\n",
    "    first = group.iloc[:2]  # Get the first available day of the month.\n",
    "    first.sort_values(first.columns[0], inplace=True)\n",
    "    last = group.iloc[-2:]  # Get the last available day of the month.\n",
    "    last.sort_values(last.columns[0], inplace=True)\n",
    "    \n",
    "    # Calculate the timespan of the available days (last - first).\n",
    "    tspan = (last.index.date[0] - first.index.date[0]).days + 1\n",
    "    \n",
    "    facc, fday = first.Totals  # fday := first day daily, facc := first day total\n",
    "    lacc, lday = last.Totals   # lday := last day daily, lacc := last day total\n",
    "    \n",
    "    # Calculate average based on timespan.\n",
    "    average = (lacc - facc) / tspan\n",
    "    \n",
    "    # If we only have a day's data available for a month,\n",
    "    # just return that day's daily count as the average.\n",
    "    return round(average, 2) if tspan > 1 else round(fday, 2)\n",
    "\n",
    "def average_on_totals_without_daily(group):\n",
    "    first = group.iloc[:1]  # Get the first available day of the month.\n",
    "    last = group.iloc[-1:]  # Get the last available day of the month.\n",
    "    \n",
    "    # Calculate the timespan of the available days (last - first).\n",
    "    tspan = (last.index.date[0] - first.index.date[0]).days + 1\n",
    "    \n",
    "    facc = first.Totals[0]\n",
    "    lacc = last.Totals[0]\n",
    "    \n",
    "    average = (lacc - facc) / tspan\n",
    "    \n",
    "    return round(average, 2) if tspan > 1 else np.NaN\n",
    "    \n",
    "\n",
    "# Calculate averages for Guinea\n",
    "print(\"---> Making calculations for Guinea\")\n",
    "# Define criteria\n",
    "cases_criteria = [x in ['ADA_CUM_CASES_TOTAL', 'ADA_NEW_CASES_TOTAL'] for x in gdf.Description]\n",
    "deaths_criteria = [x in ['ADA_CUM_DEATHS_TOTAL', 'ADA_NEW_DEATHS_TOTAL'] for x in gdf.Description]\n",
    "\n",
    "print(\"-> Calculating average cases per day for Guinea\")\n",
    "criteria = cases_criteria\n",
    "guinea_cdf = gdf.loc[criteria] \\\n",
    "                 .groupby(by=lambda x: x.strftime(\"%Y-%m\")) \\\n",
    "                 .apply(average_on_totals_with_daily)\n",
    "guinea_cdf.name = 'Cases'\n",
    "print(guinea_cdf)\n",
    "\n",
    "print(\"-> Calculating average deaths per day for Guinea\")\n",
    "criteria = deaths_criteria\n",
    "guinea_ddf = gdf.loc[criteria] \\\n",
    "                 .groupby(by=lambda x: x.strftime(\"%Y-%m\")) \\\n",
    "                 .apply(average_on_totals_with_daily)\n",
    "guinea_ddf.name = 'Deaths'\n",
    "print(guinea_ddf)\n",
    "\n",
    "print()\n",
    "\n",
    "# Calculate averages for Liberia\n",
    "print(\"---> Making calculations for Liberia\")\n",
    "# Define criteria\n",
    "cases_criteria = [x in ['ADA_CUM_CASES_TOTAL', 'ADA_NEW_CASES_TOTAL'] for x in ldf.Variable]\n",
    "deaths_criteria = [x in ['ADA_CUM_DEATHS_TOTAL', 'ADA_NEW_DEATHS_TOTAL'] for x in ldf.Variable]\n",
    "\n",
    "ldf.columns = ['Totals' if x == 'National' else x for x in ldf.columns]\n",
    "\n",
    "print(\"-> Calculating average cases per day for Liberia\")\n",
    "criteria = cases_criteria\n",
    "liberia_cdf = ldf.loc[criteria] \\\n",
    "                 .groupby(by=lambda x: x.strftime(\"%Y-%m\")) \\\n",
    "                 .apply(average_on_totals_without_daily)\n",
    "liberia_cdf.name = 'Cases'\n",
    "print(liberia_cdf)\n",
    "\n",
    "print(\"-> Calculating average deaths per day for Liberia\")\n",
    "criteria = deaths_criteria\n",
    "liberia_ddf = ldf.loc[criteria] \\\n",
    "                 .groupby(by=lambda x: x.strftime(\"%Y-%m\")) \\\n",
    "                 .apply(average_on_totals_with_daily)\n",
    "liberia_ddf.name = 'Deaths'\n",
    "print(liberia_ddf)\n",
    "\n",
    "print()\n",
    "\n",
    "# Calculate averages for Sierra Leone\n",
    "print(\"---> Making calculations for Sierra Leone\")\n",
    "# Define criteria\n",
    "cases_criteria = [x in ['ADA_CUM_CASES_TOTAL', 'ADA_NEW_CASES_TOTAL'] for x in sdf.variable]\n",
    "deaths_criteria = [x in ['ADA_CUM_DEATHS_TOTAL', 'ADA_NEW_DEATHS_TOTAL'] for x in sdf.variable]\n",
    "\n",
    "sdf.columns = ['Totals' if x == 'National' else x for x in sdf.columns]\n",
    "\n",
    "print(\"-> Calculating average cases per day for Sierra Leone\")\n",
    "criteria = cases_criteria\n",
    "sierra_cdf = sdf.loc[criteria] \\\n",
    "                .groupby(by=lambda x: x.strftime(\"%Y-%m\")) \\\n",
    "                .apply(average_on_totals_without_daily)\n",
    "sierra_cdf.name = 'Cases'\n",
    "print(sierra_cdf)\n",
    "\n",
    "print(\"-> Calculating average deaths per day for Sierra Leone\")\n",
    "criteria = deaths_criteria\n",
    "sierra_ddf = sdf.loc[criteria] \\\n",
    "                .groupby(by=lambda x: x.strftime(\"%Y-%m\")) \\\n",
    "                .apply(average_on_totals_without_daily)\n",
    "sierra_ddf.name = 'Deaths'\n",
    "print(sierra_ddf)\n",
    "\n",
    "for df in [guinea_cdf, guinea_ddf, liberia_cdf, liberia_ddf, sierra_cdf, sierra_ddf]:\n",
    "    df.index.name = 'Month'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to combine the resulting `DataFrames` containing the averages to one singe `DataFrame`. We chose to indicate missing values with `N/A`s rather than with zeroes, in order to be clear that the data is missing and not mistakenly considered zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12']\n",
    "variables = ['Cases', 'Deaths']\n",
    "countries = ['Guinea', 'Liberia', 'Sierra Leone']\n",
    "index = pd.MultiIndex.from_product((months, variables), names=('Month', 'Variable'))\n",
    "DF = pd.DataFrame(data='N/A',\n",
    "            index=index,\n",
    "            columns=countries)\n",
    "\n",
    "# Assign cases\n",
    "for df, country in [(guinea_cdf, 'Guinea'), (liberia_cdf, 'Liberia'), (sierra_cdf, 'Sierra Leone')]:\n",
    "    for month in df.index:\n",
    "        DF.loc[month, 'Cases'][country] = df[month]\n",
    "\n",
    "# Assign deaths\n",
    "for df, country in [(guinea_ddf, 'Guinea'), (liberia_ddf, 'Liberia'), (sierra_ddf, 'Sierra Leone')]:\n",
    "    for month in df.index:\n",
    "        DF.loc[month, 'Deaths'][country] = df[month]\n",
    "\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "First, we read our 9 excel files and store them into 9 different DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mid1 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID1.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid2 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID2.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid3 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID3.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid4 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID4.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid5 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID5.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid6 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID6.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid7 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID7.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid8 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID8.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)\n",
    "mid9 = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'MID9.xls'), sheetname = 'Sheet 1', index_col = 0, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give names to the index and the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mid1.index.name = mid2.index.name = mid3.index.name = mid4.index.name = 'Taxonomy'\n",
    "mid5.index.name = mid6.index.name = mid7.index.name = mid8.index.name = mid9.index.name = 'Taxonomy'\n",
    "\n",
    "mid1.columns = mid2.columns = mid3.columns = mid4.columns = ['Count']\n",
    "mid5.columns = mid6.columns = mid7.columns = mid8.columns = mid9.columns = ['Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the 9 DataFrames into a single DataFrame, on axis 0 (rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = pd.concat([mid1, mid2, mid3, mid4, mid5, mid6, mid7, mid8, mid9], axis = 0, \n",
    "                keys=['MID1', 'MID2', 'MID3', 'MID4', 'MID5', 'MID6', 'MID7', 'MID8', 'MID9'], names = [\"Barcode\"])\n",
    "\n",
    "mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(DATA_FOLDER, 'microbiome', 'metadata.xls'), sheetname = 'Sheet1', index_col = 0, header = 0)\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns using lower-case letters, except for the first letter, just so that they look prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.index.name = \"Barcode\"\n",
    "meta.rename(columns = {\"GROUP\": \"Group\", \"SAMPLE\": \"Sample\"}, inplace = True)\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the two DataFrames, mid and meta, into a single merged DataFrame and we fill all the NaN values with the tag \"unknown\". We also remove the index, so that we can reset it from scratch, after we decide exactly how we want to present our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(mid, meta, how='outer', left_index=True, right_index=True)\n",
    "# merged.index # True\n",
    "\n",
    "merged = merged.fillna(\"unknown\")\n",
    "\n",
    "merged = merged.reset_index()\n",
    "\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pivot table from the merged data, where the taxonomy is the index, the group, sample and barcode are the columns and the count is the value. We fill each blank cell with the value 0. (A blank cell does not mean that the value is not known, but rather that a particular microorganism was not encountered in a specific group of extractions, a specific barcode and with a specific sample type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use this type of representation for the data, so as to allow the reader to observe, in a single row, how much a specific microorganism has been detected in various extractions. Since a group can contain several samples and barcodes, we considered appropriate to set it as our first level of columns. Each group (apart from the one called \"EXTRACTION CONTROL\") seems to use both stool and tissue samples. Thus, we used the sample as our second level of columns. Finally, our last level of columns is the barcode, which seems to be simply an id, without carrying any significant information about the extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final = merged.pivot_table(index=['Taxonomy'], columns=['Group', 'Sample', 'Barcode'], fill_value = 0,\n",
    "                           aggfunc=np.sum, margins=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns using lower-case letters, except for the first letter, just so that they look prettier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(DATA_FOLDER+'/titanic.xls', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which columns contain at least one NaN\n",
    "df.columns[pd.isnull(df).sum() > 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  COLUMN: pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no need to analyze more the column pclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  COLUMN: survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no need to analyze more the column survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  COLUMN: name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not think that there should be any analysis on the column name, regarding the range of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  COLUMN: sex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of sex column shall be treated as categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_map = {'male': 0, 'female': 1}\n",
    "df.sex.replace(sex_map, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  COLUMN: age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.age.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = df.age.dropna()\n",
    "age.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value range of the age column goes from newborn babies(0.166) to 80 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMN: sibsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sibsp col is of type int64 and the range of values: [0 - 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sibsp.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMN: parch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parch is of type int64 and the range of values: [0 - 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.parch.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMN: ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thic column is of type object and I do not think there should not be any analysis regarding the range values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMN: fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is of type float64 and there are Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = df.fare.dropna()\n",
    "fare.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some passengers that their fare is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMN: cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cabin column is of type object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin = df.cabin.dropna()\n",
    "cabin.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLUMN: embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = df.embarked.dropna()\n",
    "emb.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passengers where embarked from three different ports\n",
    "In this case we shall use categorical data and represent {S = 0, C = 1, Q = 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_map = {'S': 0, 'C': 1, 'Q' : 2}\n",
    "df.embarked.replace(embarked_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.embarked.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the columns boat, body, home.dest there is not any range for the values that makes sense.\n",
    "Also, the data in the three columns vary, in a way that if we categorize them, it would not improve our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2\n",
    "\n",
    "Plot histograms for the travel class, embarkation port, sex and age attributes. For the latter one, use discrete decade intervals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for the travel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for the embarkation port\n",
    "\n",
    "where 0 stands for port 'S' (Southampton), 1 for 'C' (Cherbourg) and 2 for 'Q' (Queenstown) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist('embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for the sex\n",
    "\n",
    "where 0 stands for male, 1 for female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for age using discrete decade intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_range = np.arange(0, 100+10, 10)\n",
    "out, bins = pd.cut(df.age, bins = bin_range, include_lowest=True, right=False, retbins=True)\n",
    "out.value_counts(sort=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3\n",
    "\n",
    "Calculate the proportion of passengers by cabin floor. Present your results in a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cabin.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df[['pclass','cabin']].dropna(axis=0).groupby(by = df.cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to group by floor we add to the dataframe a column that descrives the floor.\n",
    "This information is obtained by the first letter of the attribute cabin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped2 = df[['pclass','cabin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped2 = df_grouped2.dropna(axis=0)\n",
    "df_grouped2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_floor(cabin):\n",
    "    return cabin[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped2['cabin_floor'] = df_grouped2['cabin'].apply(lambda x: cabin_floor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped2 = df_grouped2.drop('cabin', axis=1).groupby(by='cabin_floor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we present the proportion of passengers by cabin floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df_grouped2.count()\n",
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_c.index\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 9\n",
    "plt.subplot(aspect=1)\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.pie(df_c['pclass'], labels=labels,  autopct='%1.2f%%', shadow=True, radius=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4\n",
    "\n",
    "For each travel class, calculate the proportion of the passengers that survived. Present your results in pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_gr = df[['survived','pclass','name']].groupby(by=['pclass','survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_gr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survived = df[['pclass','survived']].groupby('pclass')['survived'].sum()\n",
    "df_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_grouped = df[['pclass','survived']].groupby('pclass')['survived'].count()\n",
    "df_class_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proportion_survived = df_survived/df_class_grouped * 100\n",
    "df_proportion_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_proportion_survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proportion_survived = df_proportion_survived.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_proportion_survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proportion_survived['not survived'] = df_proportion_survived['survived'].apply(lambda x: 100 - x)\n",
    "df_proportion_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['survived', 'not survived']\n",
    "prop = ['proportion']\n",
    "df_temp = pd.DataFrame(index=labels, columns=prop)\n",
    "#for i, row in df_proportion_survived.iteritems():\n",
    "#    df_temp.loc['survived'] = df_proportion_survived[i]\n",
    "#    df_temp.loc['not survived'] = 100 - df_proportion_survived[i]\n",
    "#    plt.subplot(aspect=1)\n",
    "#    plt.pie(df_temp, labels=labels,  autopct='%1.2f%%', shadow=True, radius=1.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 8))\n",
    "for i, (idx, row) in enumerate(df_proportion_survived.iterrows()):\n",
    "    ax = axes[i % 3]\n",
    "    row = row[row.gt(row.sum() * .01)]\n",
    "    ax.pie(row, labels=row.index, startangle=0)\n",
    "    title = 'pclass '+ str(idx)\n",
    "    ax.set_title(title)\n",
    "fig.subplots_adjust(wspace=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 5\n",
    "\n",
    "Calculate the proportion of the passengers that survived by travel class and sex. Present your results in a single histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['survived'] == 1].hist(column= 'survived', by=['sex', 'pclass'])\n",
    "#(0,1) stands for male in class 1, (0,2) for male in class 2, (1,1) for female in class 1 etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_class_sex = df.groupby(by=['pclass', 'sex'])['survived']\n",
    "df_class_sex = grouped_class_sex.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_sex = df_class_sex.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_sex.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_class_and_sex = df[df.survived == 1].pivot_table(index = 'pclass', columns = 'sex', values='survived', aggfunc='sum')\n",
    "survived_class_and_sex['Total'] = survived_class_and_sex.sum(axis=1)\n",
    "percentage = survived_class_and_sex.loc[:,[0,1]].div(survived_class_and_sex['Total'], axis=0) * 100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underline graph presents the result in a better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 6\n",
    "\n",
    "Create 2 equally populated age categories and calculate survival proportions by age category, travel class and sex. Present your results in a DataFrame with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider that age values < 1, are babies. Since their name is provided we think that removing these values from the dataframe would not be right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['age'], inplace=True)\n",
    "df.sort_values(by='age', ascending=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df['age']\n",
    "df_age = df_age.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populated = np.split(df_age,2)\n",
    "len(populated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populated[0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populated[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 2 equally age categories, they are separated on the age 28. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat1 = df.loc[:523, :]\n",
    "df_cat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.cut(df.age, bins=2, labels=['age_category1','age_category2'])\n",
    "df['age_category'] = labels\n",
    "last_df = df[df.survived == 1].pivot_table(index = ['pclass','sex','age_category'], values='survived', aggfunc='sum')\n",
    "last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = last_df.div(last_df.sum(axis=0)) * 100\n",
    "percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
